from abc import ABC, abstractmethod
import math
import logging
import pickle
import random
import numpy as np
import gymnasium as gym
from gymnasium import spaces
from psycopg.rows import dict_row
import torch
from torch.nn.functional import softmax
from sklearn.preprocessing import MinMaxScaler
from enum import unique, Enum

from envs.spaces.index_policy import IndexRepr, OneHotIndexPolicy
from envs.spaces.utils import check_subspace, fetch_server_indexes
from envs.lsc import LSC


class IndexAction(object):
    idx_type: str = None
    # Name of the table being operated on.
    tbl_name: str = None
    # Overwritten index name if relevant.
    _idx_name: str = None
    # Columns in order.
    columns: list[str] = None
    col_idxs: list[int] = None
    inc_names: list[str] = None
    # Raw action representation of the action if generated by an action.
    raw_repr = None
    bias = 0

    @property
    def is_valid(self):
        return self.tbl_name is not None and self.columns is not None and len(self.columns) > 0

    @classmethod
    def construct(cls, idx_type, tbl, columns, col_idxs, inc_names, raw_repr, index_counter, bias=0):
        ia = IndexAction()
        ia.idx_type = idx_type
        ia.tbl_name = tbl
        ia.columns = columns
        ia.col_idxs = col_idxs
        ia.inc_names = inc_names
        ia.raw_repr = raw_repr
        ia._idx_name = f"index{index_counter}"
        ia.bias = bias
        return ia

    @classmethod
    def construct_md(cls, idx_name, table, idx_type, columns, inc_names):
        ia = IndexAction()
        ia.tbl_name = table
        ia.columns = columns
        ia.idx_type = idx_type
        ia.inc_names = inc_names
        ia._idx_name = idx_name
        return ia

    def sql(self, add, allow_fail=False):
        assert self._idx_name is not None
        idx_name = self._idx_name
        if not add:
            if allow_fail:
                return f"DROP INDEX IF EXISTS {idx_name}"
            return f"DROP INDEX {idx_name}"

        return "CREATE INDEX {flag} {idx_name} ON {tbl_name} USING {idx_type} ({columns}) {inc_clause}".format(
            flag="IF NOT EXISTS" if allow_fail else "",
            idx_name=idx_name,
            tbl_name=self.tbl_name,
            idx_type=self.idx_type,
            columns=",".join(self.columns),
            inc_clause="" if len(self.inc_names) == 0 else "INCLUDE (" + ",".join(self.inc_names) + ")")

    # This equality/hash mechanism is purely based off of index identity.
    # We ensure that all other flags are exclusive from a "validity" pre-check.
    #
    # For instance, when de-duplication, one needs to check that the IndexAction
    # can *actually* be used before relying on the identity test. Can't drop an
    # index that doesn't exist; can't create an index that does for instance.
    def __eq__(self, other):
        if type(other) is type(self):
            ts = set(self.inc_names)
            os = set(other.inc_names)
            return self.idx_type == other.idx_type and self.tbl_name == other.tbl_name and self.columns == other.columns and ts == os
        return False

    def __hash__(self):
        h = hash((self.idx_type, self.tbl_name, tuple(self.columns), tuple(sorted(set(self.inc_names)))))
        return h

    def __repr__(self, add=True):
        return "{a} {idx_name} ON {tbl_name} USING {idx_type} ({columns}) {inc_clause}".format(
            a="CREATE" if add else "NOOP", #"DROP",
            idx_name=self._idx_name,
            tbl_name=self.tbl_name,
            idx_type=self.idx_type,
            columns=",".join(self.columns),
            inc_clause="" if len(self.inc_names) == 0 else "INCLUDE (" + ",".join(self.inc_names) + ")")

    def get_idx_name(self):
        assert self._idx_name is not None
        return self._idx_name


class IndexSpace(spaces.Tuple):
    # Tables in the index space.
    tables: list[str] = None
    # Maximum number of columns in a table.
    max_num_columns: int = None
    # Index representation policy.
    index_repr_policy = None
    # Sub-dimensions in the flattened.
    dims = None
    # Relation metadata.
    rel_metadata: dict[str, list[str]] = None
    class_mapping = {}
    # Indexes tracking.
    state_container: list[IndexAction] = None
    # Index counter.
    index_counter: int = 0
    # Latent Dimension.
    latent_dim: int = 0
    vae = None
    # LSC object.
    lsc: LSC = None

    index_space_aux_type_dim = None
    index_space_aux_include = None

    def save_state(self):
        return {
            "rel_metadata": self.rel_metadata,
            "class_mapping": self.class_mapping,
            "state_container": self.state_container,
            "index_counter": self.index_counter,
            "lsc": self.lsc,
        }

    def load_state(self, d):
        self.rel_metadata = d["rel_metadata"]
        self.class_mapping = d["class_mapping"]
        self.state_container = d["state_container"]
        self.index_counter = d["index_counter"]
        self.lsc = d["lsc"]

    def get_index_class(self, env_act):
        ia = self.construct_indexaction(env_act)
        if not ia.is_valid:
            return "-1"
        return str(self.class_mapping[(ia.tbl_name, ia.columns[0])])

    def get_state(self, env):
        return [ia.raw_repr for ia in self.state_container if ia.raw_repr is not None]

    def get_state_with_bias(self, env):
        if self.state_container is None:
            return []
        return [(ia.raw_repr, ia.bias) for ia in self.state_container if ia.raw_repr is not None]

    def get_latent_dim(self):
        return self.latent_dim

    def get_critic_dim(self):
        index_dim = gym.spaces.utils.flatdim(self)
        if self.latent:
            index_dim = self.get_latent_dim()
        return index_dim + self.index_space_aux_type_dim + self.index_space_aux_include

    @property
    def latent(self):
        return self.latent_dim > 0

    def _process_network_output(self, subproto):
        if self.latent:
            if self.index_output_func is not None:
                return self.index_output_func(subproto)

            return torch.tanh(subproto)

        # Otherwise process network outputs.
        return self.index_repr_policy.process_network_output(subproto)

    def _perturb_noise(self, proto, noise):
        if self.latent:
            # Similar to above reasoning.
            if self.scale_noise_perturb:
                return torch.clamp(proto + noise * self.index_output_scale, 0., self.index_output_scale)
            else:
                return torch.clamp(proto + noise, 0., 1.)
        # Otherwise perturb the noise.
        return self.index_repr_policy.perturb_noise(proto, noise)

    def _decode(self, act, select=True, override_device=None):
        if self.vae is not None:
            # Apply the shift bias before we try to decode.
            if self.lsc is not None:
                act = self.lsc.apply_current_bias(act)

            # Morph the input from the latent space through the decoder.
            # Then we pass the decoded representation through the fixer.
            device = "cuda" if torch.cuda.is_available() else "cpu"
            device = override_device if override_device else device
            act = self.vae.decoder(act.to(device=device)).cpu().detach().view(act.shape[0], -1)

            if select:
                # Only need to do additional processing if we are treating as one-hot softmax representation.
                # Now treat it as it came out of the neural network and process it.
                if len(self.tables) < self.max_num_columns + 1:
                    # Yoink only the table components that we care about.
                    distort = [l for l in range(0, len(self.tables))] + [l for l in range(self.max_num_columns + 1, act.shape[1])]
                else:
                    # Yoink only the index components that we care about.
                    distort = [l for l in range(0, len(self.tables))]
                    [distort.extend([b + i for i in range(0, self.max_num_columns + 1)]) for b in range(len(self.tables), act.shape[1], len(self.tables))]

                act = torch.index_select(act, 1, torch.tensor(distort))

            # Process the network output.
            act = self.index_repr_policy.process_network_output(act, select=select)
        return act

    def __init__(self,
            agent_type: str,
            tables: list[str],
            max_num_columns: int,
            index_repr, seed,
            latent_dim,
            index_output_scale=1.,
            index_output_func=torch.tanh,
            index_vae_config=None,
            index_vae_model=None,
            attributes_overwrite=None,
            tbl_include_subsets=None,
            lsc=None,
            scale_noise_perturb=False,
            index_space_aux_type=False,
            index_space_aux_include=False):

        self.index_space_aux_type_dim = 2 if index_space_aux_type else 0
        self.index_space_aux_include = max_num_columns if index_space_aux_include else 0
        if attributes_overwrite is not None:
            # Overwrite the maximum number of columns.
            max_num_columns = max([len(cols) for _, cols in attributes_overwrite.items()])
            self._rel_metadata = attributes_overwrite
        else:
            self._rel_metadata = None
        self.tbl_include_subsets = tbl_include_subsets

        self.tables = tables
        self.max_num_columns = max_num_columns
        self.latent_dim = latent_dim
        self.index_output_scale = index_output_scale
        self.index_output_func = index_output_func
        self.agent_type = agent_type
        self.scale_noise_perturb = scale_noise_perturb

        # Initialize the policy depending on the representation.
        self.index_repr = index_repr = IndexRepr[index_repr]
        if index_repr == IndexRepr.ONE_HOT:
            self.index_repr_policy = OneHotIndexPolicy(self.tables, max_num_columns, index_space_aux_type, self.index_space_aux_include, maximize=False)
            # Not allowed to use SAC for a sampling index interpretation.
        elif index_repr == IndexRepr.ONE_HOT_DETERMINISTIC:
            self.index_repr_policy = OneHotIndexPolicy(self.tables, max_num_columns, index_space_aux_type, self.index_space_aux_include, maximize=True)
            assert self.latent_dim > 0

        if self.latent_dim > 0:
            assert index_vae_config is not None or index_vae_model is not None
            if index_vae_model is not None:
                self.vae = index_vae_model
            else:
                from embeddings.train import _create_vae_model
                max_attrs = max_num_columns + 1
                max_cat_features = max(max_num_columns + 1, len(self.tables))
                self.vae = _create_vae_model(index_vae_config[0], max_attrs, max_cat_features)
                self.vae.load_state_dict(torch.load(index_vae_config[1], map_location="cpu"))

            device = "cuda" if torch.cuda.is_available() else "cpu"
            self.vae.to(device=device)

        # Save the LSC object.
        if lsc is not None:
            assert self.latent_dim > 0
            self.lsc = lsc

        # Acquire the internal spaces.
        internal_spaces = self.index_repr_policy.spaces(seed)
        super().__init__(spaces=internal_spaces, seed=seed)

    def construct_indexaction(self, act):
        assert check_subspace(self, act)
        # Invoke the policy to get the table and column names.
        idx_type, tbl_name, col_names, col_idxs, inc_names = self.index_repr_policy.act_to_columns(act, self.rel_metadata)
        bias = 0 if self.lsc is None else self.lsc.current_bias()
        return IndexAction.construct(idx_type, tbl_name, col_names, col_idxs, inc_names, act, self.index_counter, bias)

    def _env_to_embedding(self, env_act):
        if not isinstance(env_act, list):
            env_act = [env_act]

        if self.vae is None:
            # Just flatten.
            nets = []
            for act in env_act:
                assert check_subspace(self, act)
                nets.append(gym.spaces.utils.flatten(self, act))
            return nets
        else:
            # Marshall it through...
            device = "cuda" if torch.cuda.is_available() else "cpu"
            env_act = np.array(env_act).reshape(len(env_act), -1)
            index_type = None
            include_col = None
            if self.index_space_aux_type_dim > 0:
                # Boink the index type.
                index_val = torch.tensor(env_act[:, 0]).view(env_act.shape[0], -1)
                index_type = torch.zeros(index_val.shape[0], 2, dtype=torch.int64)
                index_type = index_type.scatter_(1, index_val, 1).type(torch.float32)
                env_act = env_act[:, 1:]

            if self.index_space_aux_include > 0:
                include_col = torch.tensor(env_act[:, -self.index_space_aux_include:]).float()
                env_act = env_act[:, :-self.index_space_aux_include]

            nets = self.vae.get_collate()(env_act).to(device=device).float()

            # There isn't much we can do if we encounter an error.
            latents, error = self.vae.latents(nets)
            latents = latents.cpu()
            assert not error

            # Apply the bias associated with generating the action.
            if self.lsc is not None:
                latents = self.lsc.apply_current_bias(latents)

            if index_type is not None:
                latents = torch.concat([index_type, latents], dim=1)
            if include_col is not None:
                latents = torch.concat([latents, include_col], dim=1)

            nets = [latent.numpy() for latent in latents]
            return nets

    def _random_embed_action(self, num_action):
        if not self.latent:
            # To acquire a random action, we just randomly distribute weights.
            return np.random.uniform(low=0., high=1., size=(num_action, gym.spaces.utils.flatdim(self)))
        else:
            # Require latent space.
            assert self.latent
            # We have an embedding so generate within the VAE KL target.

            # We don't apply the LSC constraint to the embedding here.
            # This is because we will apply the bias when we're trying
            # to decode and when trying to get the embedding from environment.
            return np.random.uniform(
                low=0.0,
                high=self.index_output_scale,
                size=(num_action, self.latent_dim,)).reshape(num_action, -1)

    def random_action_table(self, table_idx, col_idx = None, truncate_target = None):
        action = np.zeros(gym.spaces.utils.flatdim(self))
        if table_idx is None:
            # Make equal weight.
            action[0:len(self.tables)] = 1. / len(self.tables)
        else:
            # Hit only the targeted table.
            action[table_idx] = 1

        if col_idx is not None:
            action[len(self.tables) + col_idx + 1] = 1.
            # Evenly distribute the column weights.
            action[len(self.tables) + (self.max_num_columns + 1):] = 1. / (self.max_num_columns + 1)
        else:
            action[len(self.tables):] = 1. / (self.max_num_columns + 1)
        # Don't allow breaking early since we are sampling the length of the index already.
        # But only sample if truncate_target is not -1.
        if truncate_target == -1:
            return self._sample_action_distribution(torch.tensor(action), sample_num_columns=False, allow_break=False)
        return self._sample_action_distribution(torch.tensor(action), sample_num_columns=True, allow_break=False, column_override=truncate_target)

    def null_action(self):
        dim = gym.spaces.utils.flatdim(self) - self.index_space_aux_type_dim - self.index_space_aux_include
        action = np.zeros(dim)
        action[0] = 1
        action = self._sample_action_distribution(torch.tensor(action), sample_num_columns=False)
        assert self.contains(action)
        return action

    def _sample_action_distribution(self, action, sample_num_columns=False, allow_break=True, column_override=None): # Sample a real action given the network outputs.
        ret = self.index_repr_policy.sample_action(self.np_random, action, self.rel_metadata, sample_num_columns, allow_break=allow_break, column_override=column_override)
        assert check_subspace(self, ret)
        return ret

    def _sample_action_subsets(self, action, column_ordinal_mask=None):
        # Sample all subsets of an index action.
        new_candidates = self.index_repr_policy.sample_subsets(
                action,
                rel_metadata=self.rel_metadata,
                tbl_include_subsets=self.tbl_include_subsets,
                column_ordinal_mask=column_ordinal_mask)

        for act in new_candidates:
            assert check_subspace(self, act)
        return new_candidates

    def search_embedding_neighborhood(self, act, neighbor_parameters):
        # Whether we should sample the column length.
        actions_set = set()
        actions = []

        num_enter = 0
        allow_random_samples = False
        while len(actions) == 0:
            for _ in range(neighbor_parameters["index_num_samples"]):
                sampled_action = self._sample_action_distribution(act)
                assert self.contains(sampled_action)
                candidates = [sampled_action]

                if allow_random_samples:
                    # Only allow *pure* random samples once the flag has been set.
                    random_act = self._random_action(network_space)
                    assert self.contains(random_act)
                    candidates.append(random_act)

                # Sample subsets if we aren't sampling the length of the index already.
                if ("index_subset" not in neighbor_parameters) or neighbor_parameters["index_subset"]:
                    candidates.extend(self._sample_action_subsets(sampled_action))

                for candidate in candidates:
                    ia = self.construct_indexaction(candidate)
                    if ia not in actions_set:
                        # See IndexAction.__hash__ comment.
                        actions.append(candidate)
                        actions_set.add(ia)

                if len(actions) >= neighbor_parameters["index_num_samples"]:
                    # We have generated enough candidates. At least based on num_samples.
                    break

            num_enter += 1
            if num_enter >= 100:
                # Log but don't crash.
                logging.error("Spent 100 iterations and could not find any valid index action. This should not happen.")
                allow_random_samples = True
        return actions

    def _build_mapping(self, rel_metadata):
        # Deterministically construct the class mapping.
        self.class_mapping = {}
        for tbl in self.tables:
            for col in rel_metadata[tbl]:
                self.class_mapping[(tbl, col)] = len(self.class_mapping)

    def reset(self, **kwargs):
        # Advance the counter.
        self.index_counter += 1

        if kwargs.get("no_lsc", False):
            # This is a "soft-reset" by which we've only just restarted the instance.
            # In this case, don't actually do anything!
            return

        # Reset the LSC.
        if self.lsc is not None:
            self.lsc.reset()

        assert "connection" in kwargs
        connection = kwargs["connection"]

        # Get the metadata and indices.
        self.rel_metadata, existing_indexes = fetch_server_indexes(connection, self.tables)
        if self._rel_metadata is not None:
            # Validate the relation metadata that we were provided.
            for tbl, cols in self._rel_metadata.items():
                if tbl == "actual":
                    continue

                assert tbl in self.rel_metadata
                for col in cols:
                    assert col in self.rel_metadata[tbl]
            self._rel_metadata["actual"] = self.rel_metadata
            self.rel_metadata = self._rel_metadata
        self._build_mapping(self.rel_metadata)

        # Construct the actual IndexActions.
        indices_check = []
        if "config" in kwargs and kwargs["config"] is not None:
            # FIXME:
            target_config = kwargs["config"][-1]

            old_container = self.state_container
            for act in target_config:
                ia = self.construct_indexaction(act)
                if ia in old_container:
                    # Fetch the bias if relevant.
                    indices_check.append(old_container[old_container.index(ia)])
                else:
                    indices_check.append(ia)

        self.state_container = []
        for relname, indexes in existing_indexes.items():
            for idxname, idx_desc in indexes.items():
                idx_type = idx_desc["index_type"]
                col_repr = ",".join(idx_desc["columns"])
                inc_col_repr = ("INCLUDE (" + ",".join(idx_desc["include"]) + ")") if len(idx_desc["include"]) > 0 else ""

                logging.debug(f"Existing index: {idxname} ON {relname} USING {idx_type} ({col_repr}) {inc_col_repr}")

                ia = IndexAction.construct_md(idxname, relname, idx_type, idx_desc["columns"], idx_desc["include"])
                if ia in indices_check:
                    # Keep the old index action if the index already exists.
                    self.state_container.append(indices_check[indices_check.index(ia)])
                else:
                    self.state_container.append(ia)

    def advance(self, action, **kwargs):
        # Construct the index action.
        ia = self.construct_indexaction(action)

        # Advance the counter.
        self.index_counter += 1

        # Advance the LSC.
        if self.lsc is not None:
            self.lsc.advance()

        if not ia.is_valid:
            # This is the case where the action we are taking is a no-op.
            return

        if ia not in self.state_container:
            self.state_container.append(ia)

    def generate_action_plan(self, action, **kwargs):
        assert check_subspace(self, action)

        sql_commands = []
        ia = self.construct_indexaction(action)
        if not ia.is_valid:
            # This is the case where the action we are taking is a no-op.
            return [], []

        exist_ia = ia in self.state_container
        if exist_ia:
            logging.info("Contemplating %s (exist: True)", self.state_container[self.state_container.index(ia)])
        else:
            logging.info("Contemplating %s (exist: False)", ia)
            # Add the new index with the current index counter.
            sql_commands.append(ia.sql(add=True))

        if kwargs is not None and "reset" in kwargs:
            # Advance the counter.
            self.index_counter += 1

        return [], sql_commands

    def generate_delta_action_plan(self, action, **kwargs):
        assert isinstance(action, list)
        acts = []

        allow_fail = False
        if kwargs is not None and "load" in kwargs:
            allow_fail = kwargs["load"]

        sql_commands = []
        for act in action:
            assert check_subspace(self, act)
            ia = self.construct_indexaction(act)
            acts.append(ia)

            if not ia.is_valid:
                # This is the case where the action we are taking is a no-op.
                continue

            if ia not in self.state_container:
                sql_commands.append(ia.sql(add=True))

            if kwargs is not None and "reset" in kwargs:
                # Advance the counter.
                self.index_counter += 1

        for ia in self.state_container:
            # Drop the index that is no longer needed.
            if ia not in acts:
                sql_commands.append(ia.sql(add=False, allow_fail=allow_fail))

        return [], sql_commands

    def to_jsonable(self, sample_n):
        # Emit the representation of an index.
        ias = [self.construct_indexaction(sample) for sample in sample_n]
        return [ia.__repr__(add=not ia in self.state_container) for ia in ias]
